\chapter{Solving eigenvalue problems}

The purpose of this chapter is to show how easily one can define and solve eigenvalue problems using \ARPP{} classes. There is no intent to cover every single \ARPP{} detail here, but only to stress the most important characteristics of each kind of class and function, and give some hints that should be followed by the user when solving his own problems.

\section{Solving problems in four steps}

As emphasized in chapter 3, \ARPP{} has a large number of classes and functions. This profusion of classes is easy to justify, since it gives the user various alternatives to define and solve eigenvalue problems without having to pass extra parameters when calling constructors

However, one can easily get confused when so many choices are available, especially when using the library for the first time. Therefore, the actions needed to define and solve an eigenvalue problem using a few simple steps shall be emphasized:

\begin{description}
	\item[Step One.] First of all, it is necessary to create one or more matrices using some user-defined class or one of the eight matrix classes provided by \ARPP{}. If the user does not want to represent a matrix by means of a class, he still can use the \textit{reverse communication interface}, but this option is not recommended and should be considered only after discarding the previous alternatives.
	\item[Step Two.] Once available, these matrices must be used to declare the eigenvalue problem. Other relevant parameters, such as the number of desired eigenvalues, the spectral transformation and the shift, should also be defined.
	\item[Step Three.] After that, the user must call one of the \ARPP{} functions specifically designed to solve the eigenvalue problem. \texttt{EigenValVectors} and \texttt{FindEigenvectors} are just two of these functions.
	\item[Step Four.] Finally, some other \ARPP{} function can also be called to retrieve output data, such as eigenvalues, eigenvectors and Schur vectors, if this was not done by the function used in the third step above.
\end{description}

Notice that several functions mentioned in the last chapter were not included in these steps. Functions whose purpose is to change some of the problem parameters (such as the tolerance or the maximum number of iterations) or turn on the trace mode, for example, are seldom used and, because of their secondary role, will be described only in the appendix.

\section{Defining matrices}

From the user’s point of view, the hardest step in the list given above is the definition of the matrices that characterize the eigenvalue problem. This is particularly true when the problem is large.

The difficulty comes from the fact that, in order to define a matrix, it is necessary not only to store its elements, but also to create a function that performs a matrix-vector product and, in the case of a spectral transformation is being used, to define how a linear system should be solved.

Two different schemes are provided by \ARPP{} to mitigate this difficulty: one can use a predefined class, and let the library handle the matrices, or use his own class to define the required matrix-vector products.

In fact, the reverse communication interface can also be used, so it is even possible to avoid completely the use of a c++ class to store information about the matrix. In this case, the user is totally free to decide how matrix-vector products should be performed. However, because this liberty implies a much more complicated code, only the first two alternatives will be considered in this section.

\section{Using ARPACK++ matrix classes}

The easiest way to define a matrix is to use one of the eight predefined classes provided by \ARPP{}. These classes already contain member functions that perform matrix-vector products and solve linear systems, so the user needs only to supply matrix data in compressed sparse column or band format in order to use them to define a matrix. A single vector will suffice if the matrix is dense.

The first example below illustrates how a real nonsymmetric band matrix can be declared as an \texttt{ARbdNonSymMatrix} object.
\begin{cppcode}
int     n     = 10000;
int     nL    = 6;
int     nU    = 3;
double* nzval = MatrixData();

ARbdNonSymMatrix<double, double> A(n, nL, nU, nzval);
\end{cppcode}

In this example, \texttt{n} is the dimension of the system, \texttt{nL} and \texttt{nU} are, respectively, the lower and the upper bandwidth (not considering the main diagonal), and \texttt{nzval} is a vector that contains all elements of the (\texttt{nL}+\texttt{nU}+1) nonzero diagonals of \texttt{A}. This last vector is generated by function \texttt{MatrixData} (not shown here).

Once declared, \texttt{A} can be passed as a parameter to all \ARPP{} classes that define a nonsymmetric eigenvalue problem. However, since class \texttt{ARbdNonSymMatrix} (like all other predefined matrix classes) uses a direct method to solve linear systems, one must take in account the memory that will be consumed if a spectral transformation is employed.

The next example contains the definition of a sparse complex matrix using class \texttt{ARluNonSymMatrix}.

\begin{cppcode}
int                n;     // Matrix dimension.
int                nnz;   // Number of nonzero elements in A.
int*               irow;  // pointer to an array that stores the row
                          // indices of the nonzeros in A.
int*               pcol;  // pointer to an array of pointers to the
                          // beginning of each column of A in nzval.
arcomplex<double>* nzval; // pointer to an array that stores the
                          // nonzero elements of A.

n = 10000;
CompMatrixA(n, nnz, nzval, irow, pcol);
ARluNonSymMatrix<arcomplex<double>, double> A(n, nnz, nzval, irow, pcol);
\end{cppcode}

Here, \texttt{CompMatrixA} is a function that generates \texttt{nnz}, \texttt{irow}, \texttt{pcol} and \texttt{nzval}. These four parameters, along with \texttt{n}, are used to define matrix \texttt{A}. In addition to them, some other parameters not shown here, such as the relative pivot tolerance and the column ordering that should be used to reduce the fill-ins that occur during the matrix factorization, can also be passed to the \texttt{ARluNonSymMatrix} class constructor.

\subsection{Letting the user define a matrix class}

If none of the matrix classes mentioned above meets the user’s requirements, either because the data structure is not appropriate or due to the use of a direct method for solving linear systems, a new class can be defined from scratch. In this case, the class must contain some member functions that performs the matrix-vector products required by the Arnoldi method.

Different classes with particular member functions must be created for each combination of matrix (real symmetric, real nonsymmetric or complex) and computational mode (regular, shift and invert, etc) used to solve the eigenvalue problem. To solve, in regular mode, a standard eigenvalue problem that involves a real nonsymmetric matrix $A$, for example, one needs to define a matrix class with at least one member function that performs the product $w \leftarrow Av$, as shown below.

\begin{cppcode}
template<class T>
class MatrixWithProduct {
	private:

	int m, n;   // Number of rows and columns.

	public:

	int nrows() { return m; }

	int ncols() { return n; }

	void MultMv(T* v, T* w);   // Matrix-vector product: w = M*v.

	MatrixWithProduct(int nrows, int ncols = 0) // Simple constructor.
	{
		m = nrows;
		n = (ncols?ncols:nrows);
	} 
}; // MatrixWithProduct.
\end{cppcode}

The only condition imposed to this class by \ARPP{} is that \texttt{MultMv}, the function that performs the required matrix-vector product, contains only two parameters\footnote{Naturally, default arguments are also allowed.}. The first parameter must be a pointer to the vector that will be multiplied by $A$, while the second parameter must supply a pointer to the output vector. This is not a very stringent restriction since any other information about the matrix, such as the number of rows or columns, can be passed indirectly to \texttt{MultMv} through class variables.

In the example above, parameters v and w are declared as pointers to a certain type T, allowing \texttt{MatrixWithProduct} to represent both single and double precision matrices. Other two variables used by \texttt{MultMv}, m and n, are defined when the constructor is called.

\section{Creating eigenvalue problems}

There are two different ways to declare an eigenvalue problem as an \ARPP{} object. The user can either define all problem parameters when creating the object or use a default constructor and define parameters later. Both alternatives are briefly described below

\subsection{Passing parameters to constructors}

All information that is necessary to set up the eigenvalue problem can be specified at once when declaring an object of the corresponding class. For example, to find the five eigenvalues closest to $5.7 + 2.3i$ of a complex generalized problem using the shift and invert mode, the user can declare an object of class \texttt{ARCompGenEig} writing

\begin{cppcode}
ARCompGenEig<double, MatrixOP<double>, MatrixB<double> > 
	EigProb(10000, 5, &OP, &MatrixOP<double>::MultVet, &B,
		&MatrixB<double>::MultVet, arcomplex<double>(5.7, 2.3));
\end{cppcode}

Here, $10000$ is the dimension of the system and \texttt{MatrixOP<double>::MultVet} and \texttt{MatrixB<double>::MultVet} are functions that evaluate the products $(A-\sigma B)^{-1}v$ and $Bv$, respectively.

The same complex problem mentioned above can be declared in a more straightforward way if the \texttt{ARluCompGenEig} class is used. In this case, after defining A and B as two \texttt{ARluNonSymMatrix} objects, one just needs to write

\begin{cppcode}
ARluCompGenEig<double> EigProb(5, A, B, arcomplex<double>(5.7, 2.3));
\end{cppcode}

Real symmetric and nonsymmetric standard and generalized problems can be created in an analogous manner.

\subsection{Defining parameters after object declaration}

There are some cases where it is not necessary, and sometimes not even convenient, to supply all problem information when declaring an \ARPP{} object. If some parameter is not available when problem is being declared, for example, all data can be passed to \ARPP{} later, as in the following real nonsymmetric generalized problem:

\begin{cppcode}
ARNonSymGenEig<double, MatrixOP<double>, MatrixB<double> > EigProb;

// ... 

EigProb.DefineParameters(100, 4, &OP, &MatrixOP<double>::MultVet, 
	&B, &MatrixB<double>::MultVet);
EigProb.SetShiftInvertMode(1.2, &OP, &MatrixOP<double>::MultVet);
\end{cppcode}

In this example, the shift and invert mode will be used to find $4$ eigenvalues near $1.2$. The dimension of the problem is $100$ and matrix-vector products are functions of classes \texttt{MatrixOP<double>} and \texttt{MatrixB<double>}. The first line only declares an object called \texttt{EigProb}. In the last two lines, all \texttt{ARNonSymGenEig} parameters are defined, including the spectral transformation mode.

\section{Solving problems and getting output data}

Once declared an eigenvalue problem, \ARPP{} provides several alternatives to retrieve its solution. These alternatives are briefly described below.

\subsection{Letting ARPACK++ handle data}

When solving an eigenvalue problem, \ARPP{} can hold the output vectors in its own data structure, so the user does not need to decide where they should be stored. In this case, each single element of the eigenvalues and eigenvectors can be recovered later using some functions provided by \ARPP{}, as in the following example:

\begin{cppcode}
// Finding and printing a few eigenvectors of EigProb.
	
EigProb.FindEigenvectors();
	
for (int i=0; i<EigProb.ConvergedEigenvalues(); i++) {
	cout << "Eigenvalue[" << (i+1) << "] = ";
	cout << EigProb.Eigenvalue(i) << endl;
	cout << "Eigenvector[" << (i+1) << "] : ";
	for (j=0; j<EigProb.GetN(); j++) {
		cout << EigProb.Eigenvector(i, j) << endl;
	}
	cout << endl;
}
\end{cppcode}

Here, \texttt{FindEigenvectors} is a function that determines eigenvalues and eigenvectors of a problem defined by \texttt{EigProb}, and store them into \ARPP{} internal structure. \texttt{ConvergedEigenvalues} returns the number of eigenvalues found by \ARPP{}. To retrieve output data, functions \texttt{Eigenvalue} and \texttt{Eigenvector} were used\footnote{\texttt{SchurVector} and \texttt{ResidualVector} are other functions that could be used here.}.

\ARPP{} also includes other functions that return vector addresses instead of vector elements. These functions provide direct access to output data without requiring the user to create a vector. They are well suited to those situations where eigenvalues and eigenvectors are to be supplied as parameters to other functions. \texttt{RawEigenvector}, one of such functions\footnote{Other functions with similar meaning are \texttt{RawEigenvalues} and \texttt{RawSchurVector}.}, is used in the example below:

\begin{cppcode}
// ...
EigProb.FindEigenvectors();             // Finding eigenvectors.
double* w = new double[EigProb.GetN()]; // Defining a vector w.
A.MultMv(EigProb.RawEigenvector(0), w); // Setting w <- matrix*Eigenvector
// ...
\end{cppcode}

In this example, A, a matrix declared elsewhere in the program, is multiplied by the first eigenvector of an eigenvalue problem defined by \texttt{EigProb}. The function that performs the product, \texttt{A.MultMv}, takes two pointers to double precision real vectors as parameters. \ARPP{} function \texttt{GetN} is used to determine the dimension of the problem.

\subsection{Employing user-defined data structure}

\ARPP{} also allows the user to use his own vectors to store the solution of an eigenvalue problem. As an example, a function called \texttt{EigenValVectors} is used below to determine the \texttt{nconv} eigenvalues and eigenvectors of a real nonsymmetric standard problem (represented by \texttt{EigProb}). Similar functions can be used to find Arnoldi basis vectors, Schur vectors, etc.

\begin{cppcode}
double EigValR[10];
double EigValI[10];
double EigVec[1100];
int    nconv;

nconv = EigProb.EigenValVectors(EigVec, EigValR, EigValI);
for (int i=0; i<nconv; i++) {
	cout << "Eigenvalue[" << (i+1) << "] = ";
	cout << EigValR[i] << " + " << EigValI[i] << "I" << endl;
}
\end{cppcode}

Since \texttt{EigProb} is a nonsymmetric problem and, in this case, some of the eigenvalues can be complex, two real vectors, \texttt{EigValR} and \texttt{EigValI}, are used to store, respectively, the real and imaginary part of the eigenvalues.

The eigenvectors are stored sequentially in \texttt{EigVec}. For real nonsymmetric problems, real eigenvectors occupy n successive positions\footnote{Here, n is the dimension of the eigensystem.}, while each complex eigenvector require $2\cdot$\texttt{n} positions (\texttt{n} for the real part and another \texttt{n} for the imaginary part of the vector). Since the last eigenvector found by \texttt{EigenValVectors} can be complex, \texttt{EigVec} must be dimensioned to store (\texttt{nconv}+1)$\cdot$\texttt{n} real elements.

\subsection{Using the STL vector class}

Last but not least, \ARPP{} can store eigenvalues and eigenvectors using the \texttt{vector} class provided by the Standard Template Library (or STL).

STL is a library that provides an easy and powerful way to handle vectors, linked lists and other structures in c++. Among its class templates, only the \texttt{vector} class can be considered appropriate to store the dense vectors generated as output by \ARPP{}. This class is used in the example below:

\begin{cppcode}
vector<double>* EigVec = prob.StlEigenvectors();
vector<double>* EigVal = prob.StlEigenvalues();

for (int i=0; i<prob.ConvergedEigenvalues(); i++) {
	cout << "Eigenvalue[" << (i+1) << "] = " << EigVal[i] << endl;
}
\end{cppcode}

In this example, \texttt{StlEigenvectors} not only finds the eigenvectors of a problem called \texttt{prob}, but also creates a new object of class \texttt{vector} to store them sequentially, returning a pointer to this vector in \texttt{EigVec}. \texttt{EigVal} is used to store the pointer generated by \texttt{StlEigenvalues}. The number of eigenvalues found by \ARPP{} is supplied by function \texttt{ConvergedEigenvalues}. 

\section{Finding singular values}

\ARPP{} can also be used to find the \textit{truncated singular value decomposition (truncated SVD)} of a generic real rectangular matrix. Supposing, for example, that $A$ is a $m\times x$ matrix, the truncated SVD is obtained by decomposing $A$ into the form 
\[A=U\Sigma V^{T} \] 
where $U$ and $V$ are matrices with orthonormal columns, $U^{T} U=V^{T} V=I_{n}$, and $\Sigma =diag(\sigma_{1} ,\sigma_{2} ,\ldots ,\sigma_{n} )$ is a diagonal matrix that satisfies $\sigma_{1} \ge \sigma_{2} \ge \cdots \ge \sigma_{n} \ge 0$.

Each element $\sigma_{i} $ is called a \textit{singular value} of $A$, while each column of $U$ is a \textit{left singular vector} and each column of $V$ is a \textit{right singular vector} of $A$.

To use \ARPP{} to obtain a few singular values (and the corresponding singular vectors) of $A$, one should notice that $\sigma_{1} ,\sigma_{2} ,\ldots ,\sigma_{n} $ are precisely the square roots of the eigenvalues of the symmetric $n\times n$ matrix
\[A^{T} A=V\Sigma U^{T} U\Sigma V^{T} =V\Sigma ^{2} V^{T} \] 
and, in this case, the eigenvectors of $A^{T} A$ are the right singular vectors of $A$. 

Naturally, this formulation is appropriate when $m$ is greater or equal to $n$. To solve problems where $m<n$, it is sufficient to reverse the roles of $A$ and $A^T$ in the above equation.

When the singular values obtained by \ARPP{} are not multiple or tightly clustered, numerically orthogonal left singular vectors may also be computed from the right singular vectors using the relation:
\[U=AV\Sigma ^{-1}.\] 
As an alternative, one can use the relation 
\[\left(\begin{array}{cc} {0} & {A} \\ {A^{T} } & {0} \end{array}\right)\left(\begin{array}{c} {U} \\ {V} \end{array}\right)=\left(\begin{array}{c} {U} \\ {V} \end{array}\right)\Sigma .\] 
to determine the left and right leading singular vectors simultaneously. In this case, no transformation is required since the columns of $U$ and $V$ can be easily extracted from the converged eigenvectors of 
\[\bar{A}=\left(\begin{array}{cc} {0} & {A} \\ {A^{T} } & {0} \end{array}\right).\] 
In view of the fact that $\bar{A}$ has both $\sigma_{i} $ and $-\sigma_{i} $ as eigenvalues, it is important to set variable \texttt{which} to ``LA'' when calling \ARPP{}, so only the positive eigenvalues (those with largest algebraic value) are computed.

The major drawback of this approach is related to the fact that $\bar{A}$ is a $(m+n)\times (m+n)$ matrix, while $A^{T} A$ contains only $n^{2} $ elements. Even considering that the sparse matrix-vector products $\bar{A}v$ and $A^{T} Av$ require the same amount of float point operations, the Arnoldi vectors generated at each iteration of \ARP{} are greater when $\bar{A}$ is used. Moreover, setting \texttt{which} to ``LM'' is generally better than using ``LA''. 

As a result, in most cases it is better to use $A^{T} A$ than $\bar{A}$. Exceptions to this rule occur only when the leading eigenvalues of $A$ are very tightly clustered.

\subsection{Using the ARSymStdEig class}

\ARPP{} class \texttt{ARSymStdEig} can be easily adapted to solve SVD problems. This is particularly true if \texttt{ARluNonSymMatrix}, \texttt{ARumNonSymMatrix}, \texttt{ARdsNonSymMatrix} or \texttt{ARbdNonSymMatrix} is used to store matrix A, because these classes contain three member functions, \texttt{MultMtMv}, \texttt{MultMMtv} and \texttt{Mult0MMt0}, that perform, respectively, the products $A^{T}Av$, $AA^{T}v$ and $\bar{A}v$ for a given $v$.

Supposing, for example, that vectors \texttt{valA}, \texttt{irow} and \texttt{pcol} are used to store $A$ in CSC format, so \texttt{ARluNonSymMatrix} can be used to define the matrix, the following commands are sufficient to find the four leading singular values of $A$.
\begin{cppcode}
// Using ARluNonSymMatrix to store matrix A and to perform the product 
// A'Ax (LU decomposition is not used, so SuperLU is not required).

ARluNonSymMatrix<double, double> A(m, n, nnz, valA, irow, pcol);

// Defining the eigenvalue problem (MultMtM is used, so m >= n).

ARSymStdEig<double, ARluNonSymMatrix<double, double> >
prob(n, 4, &A, &ARluNonSymMatrix<double, double>::MultMtMv);

// Finding eigenvalues.

double svalue[4];
dprob.Eigenvalues(svalue);

// Calculating the singular values.

for (i = 0; i < prob.ConvergedEigenvalues(); i++) {
	svalue[i] = sqrt(svalue[i]);
}
\end{cppcode}
Other interesting examples where \ARPP{} is used to find singular values and vectors can be found in the \texttt{arpack++/examples} directory.
